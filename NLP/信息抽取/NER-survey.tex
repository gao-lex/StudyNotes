\documentclass[a4paper,UTF8,no-math]{ctexart}
%\documentclass[a4paper,UTF8,no-math,zihao=-4]{ctexart}
%\usepackage[backref]{hyperref} 
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{zhnumber}
\usepackage{amsmath}% eqref 
\usepackage{amssymb}% mathbb数学符号
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{natbib}
%\usepackage[linkcolor=green,citecolor=blue,anchorcolor=red]{hyperref}
\usepackage{hyperref}
% \usepackage{}
\bibliographystyle{plainnat}

\setmainfont{Times New Roman}
\hypersetup{colorlinks=true}

%\hypersetup{hidelinks,bookmarks=true,bookmarksopen=true}

\geometry{a4paper,left=1.27cm,right=1.27cm,top=1.27cm,bottom=1.27cm}
\lstset{ % General setup for the package
	language=Python,
	numbers=left,
	frame=shadowbox,
	tabsize=4,
	breaklines=true, 
}
% \hypersetup{
% 	colorlinks=true
% }

\title{NER调研}
\author{高磊}
\date{\zhtoday}

\begin{document}
	% \maketitle
	\tableofcontents
	
	\newpage
	
	\section{NER}
	命名实体识别这个术语首次出现在MUC-6（Message Understanding Conferences），这个会议关注的主要问题是信息抽取（Information Extraction），即从报章等非结构化文本中抽取关于公司活动和国防相关活动的结构化信息（人名、地名、机构名、时间、数字等）。因此MUC-6组织的一项评测任务就是从文本中识别这些实体指称及其类别，这个任务叫命名实体识别和分类（named entity recognition and classification，NERC）。也就是识别实体指称的边界和类别。
	
	根据美国NIST自动内容抽取（automatic content extraction，ACE）评测计划的解释，实体概念在文本中的引用（entity mention，或称“指称项”）有三种形式：命名性指称、名词性指称和代词性指称。eg：“[[中国]乒乓球男队主教练][刘国梁]出席了会议，[他]指出了当前经济工作的重点。”中，实体“刘国梁”的指称项有三个，其中，“中国乒乓球男队主教练”是名词性指称，“刘国梁”是命名性指称，“他”是代词性指称。
	
	    命名实体识别是多种自然语言处理技术的重要基
	础，对于句法分析、语法分析、语义分析等都有着极其
	重要的影响，主要应用在信息抽取、机器翻译、问答系
	统等方面。
	
	在NER中应用的技术主要有五种\citep{nadeau2007survey}，\citep{li2018survey}，\citep{宗成庆2013统计自然语言处理}：
	
	\begin{enumerate}
		\item Rule-based approaches：这种方法不需要标注数据，依赖于手工设计的特征；
		\item Semi-supervised learning approaches：半监督的学习方法（弱监督的学习方法）利用标注的小数据集（种子数据）自举学习；
		\item Unsuperised learning approaches：依赖于无需手动标记训练样例的无监督算法；
		\item Feature-based Superised learning approaches：依赖于需要特征工程的监督算法；
		\item Deep-learning based approaches：通过端到端的方法，自动从原始数据中发掘分类and/or检测所需要的表征。
	\end{enumerate}
	
	现在较为流行的是将其中两种
	方法结合甚至是三种结合，可以充分利用不同方法的
	优点，提高学习的准确度和效率。
	
	
	\section{Rule-based approaches}
	
	这种方法依赖手工设定的规则。可以参照\citep{rau1991extracting}从文本中识别和抽取公司名称的研究。
	
	基于规则的方法多采用语言学专家手工构造规则
	模板。
	一般来说，规则的构造需要考虑到该领域的关
	键字、指示词、中心词、前后缀等特征，依赖于已制定的
	词典和知识库，通过模式匹配或字符串匹配等方法来
	识别出命名实体。其中，词典负责已有词汇的识别，规
	则负责未登录词的识别。
	
	
	在基于规则的方法中，命名实体识别使用的不仅有各种
	命名实体的构成规则，还有实体本身和上下文的关系以及用
	词情况。比较简单的中文命名实体构成规则可以举例如下:
\begin{align*}
& \text{组织名} \to  \text{([人名][组织名][地名])*[组织类型]<指示词>}\\
& \text{人名} \to  \text{<姓氏><名字>}\\
& \text{地名} \to  \text{<名字部分>*<指示词>}
\end{align*}
	这些规则往往依赖于具体语言、领域和文本风格，
	编制过程耗时且难以涵盖所有的语言。特别容易
	产生错误，系统可移植性不好。对于不同的系统需要语
	言学专家重新书写规则。基于规则的方法的另外一个
	缺点是代价太大，存在系统建设周期长、移植性差而且
	需要建立不同领域知识库作为辅助以提高系统识别能力等问题。
	
	    
	
%	规则可以根据特定领域的名词录和句法-词法来设计。这样设计的规则可以提升召回率，但是对精确度没什么作用。
%	
%	规则也可以根据主要依赖于手工制作的语义和句法特征。当专有名词很详尽的时候，这些系统表现得不错。但是这种系统使用了领域特定得规则和不完整的字典，这种系统常常有着较高的精确度但是召回率确很低，与此同时这种系统的鲁棒性不好，移植的代价很高。
	
	总之这种系统的实现代价较高，而且其可移植性很差。
	
	\newpage
	
%	Typical systems are Univ. of Sheffield's LaSIE-II [Humphreys+98], ISOQuest's NetOwl [Aone+98] [Krupha+98] and Univ. of Edinburgh's LTG [Mikheev+98] [Mikheev+99] for English NER. These systems are mainly rule-based.
%	
%	[Humphreys+98] K. Humphreys, R. Gaizauskas,S. Azzam, C. Huyck, B. Mitchell, H. Cunningham,Y. Wilks. Univ. of Sheffield: Description of the LaSIE-II System as Used for MUC-7. MUC-7 . Fairfax, Virginia. 1998.
%	
%	
%	[Aone+98] C. Aone, L. Halverson, T. Hampton,
%	M. Ramos-Santacruz. SRA: Description of the IE2
%	System Used for MUC-7. MUC-7 . Fairfax, Virginia.
%	1998.
%	
%	[Mikheev+98] A. Mikheev, C. Grover, M.
%	Moens. Description of the LTG System Used for
%	MUC-7. MUC-7 . Fairfax, Virginia. 1998.
	
%	\newpage
	
	\section{Feature-based supervised learning approaches}
	
	监督学习算法系统通常读取大规模的标注语料对模型进行参数训练。这种方法主要包括：隐马尔可夫模型（Hidden Markov Model，HMM）、最大熵模型（Maximum Entropy，ME）、支持向量机（Support Vector Machine，SVM）、条件随机场（Conditional Random Fields，CRF）等。
	
	监督学习方法的baseline取决于vocabulary transfer，即在训练语料库和测试语料库中不重复出现的单词比例。监督学习的方法对特征选取的要求较高，对特征选区很敏感。比较经典的方法主要有：隐马尔可夫模型和条件随机场。
	
	\subsection{隐马尔可夫模型HMM}
	
	% \begin{quote}
	
	% \end{quote}
	
	\textcolor{red}{这部分的知识我参考了}\citep{李航2012统计学习方法}，\citep{宗成庆2013统计自然语言处理}。

	
	隐马尔可夫模型（Hidden Markov Model，HMM），是可用于标注问题的统计学模型，描述有隐藏的马尔可夫链随机生成观测序列的过程，属于生成模型。隐马尔可夫问题可以用于标注，此时状态对应着标记。
	
	
	
	隐马尔可夫模型可表示为一个五元组$\lambda=(N,M,\mathbf{A}, \mathbf{B}, \mathbf{\pi})$，为了简便也可表示为$\lambda=(\mathbf{A}, \mathbf{B}, \mathbf{\pi})$。
	
	% \begin{enumerate}
	
	% \end{enumerate}
	
	\begin{itemize}
		\item N:状态的有限集合。在这里，是指每一个词语背后的标注；
		\item M:观察值的有限集合。在这里，是指每一个词语本身；
		\item A:状态转移概率矩阵。在这里，是指某一个标注转移到下一个标注的概率；
		\item B:观测概率矩阵，也就是发射概率矩阵。在这里，是指在某个标注下，生成某个词的概率；
		\item π:初始概率矩阵。在这里，是指每一个标注的初始化概率。
	\end{itemize}
	
	HMM有三个基本问题：
	
	\begin{enumerate}
		\item 估计问题：给定一个观察序列$O = (O_{1},O_{2}, \cdots ,O_{T})$和模型$\lambda=(\mathbf{A}, \mathbf{B},\mathbf{\pi})$，求$P(O | \lambda)$；
		\item 序列问题：给定一组观察序列和模型，求解给定观测序列条件概率最大的状态序列了；
		\item 训练问题或参数估计问题：给定观察序列，根据最大似然法来估计模型的参数。
	\end{enumerate}
	
	隐马尔可夫模型有三种应用场景，我们做命名实体识别只用到其中的一种——求观察序列的背后最可能的标注序列\footnote{这个问题也叫预测或解码(decoding)}。即给定观测序列$\mathbf{O} = (O_{1},O_{2}, \cdots ,O_{T})$，求$P(I|O)$最大的状态序列$\mathbf{I}$。
	
	在已知观测序列列$O = (O_{1},O_{2}, \cdots ,O_{T})$和模型$\lambda=(\mathbf{A}, \mathbf{B},\mathbf{\pi})$\footnote{模型的参数学习可以通过EM算法实现}，应用维特比（viterbi）算法，就可以求给定观测序列条件概率最大的状态序列了。
	
	为此，引入两个变量$\delta$和$\psi$。定义在时刻$t$状态为$i$的所有单个路径$\left(i_{1}, i_{2}, \cdots, i_{t}\right)$中概率最大值为$$\delta_{t}(i)=\max _{i_{1}, i_{2}, \cdots, i_{-1}} P\left(i_{t}=i, i_{t-1}, \cdots, i_{1}, o_{t}, \cdots, o_{1} | \lambda\right), \quad i=1,2, \cdots, N$$
	
	定义在时刻$t$为状态$i$的所有单个路径中概率最大的路径的第$t-1$个节点为：$$\psi_{t}(i)=\arg \max _{1 \leq j \leqslant N}\left[\delta_{t-1}(j) a_{j t}\right], \quad i=1,2, \cdots, N$$
	
	\begin{algorithm}
		\caption{维特比}
		\label{alg:viterbi}
		{\bf 输入：}模型$\lambda$和观测$O = (O_{1},O_{2}, \cdots ,O_{T})$
		
		{\bf 输出：}最优化路径$I^{*} = (i^{*}_{1},i^{*}_{1},\cdots,i^{*}_{n})$
		
		\begin{algorithmic}[1] 
			\STATE {初始化$$\begin{array}{c}{\delta_{1}(i)=\pi_{i} b_{i}\left(o_{1}\right), \quad i=1,2, \cdots, N} \\ {\psi_{1}(i)=0, \quad i=1,2, \cdots, N}\end{array}
				$$}
			\STATE{递推，对于$t=2,3, \cdots, T$ $$\delta_{t}(i)=\max _{1 \leqslant j \leqslant N}\left[\delta_{t-1}(j) a_{j i}\right] b_{i}\left(o_{t}\right), \quad i=1,2, \cdots, N$$ $$\psi_{t}(i)=\arg \max _{1 \leqslant j \leqslant N}\left[\delta_{t-1}(j) a_{j i}\right], \quad i=1,2, \cdots, N$$}
			\STATE{终止$$\begin{aligned} P^{*} &=\max _{1 \leqslant i \leqslant N} \delta_{T}(i) \\ i_{T}^{*}=& \arg \max _{1 \leqslant i \leqslant N}\left[\delta_{T}(i)\right] \end{aligned}$$}
			\STATE{最有路径回溯，对于$$t=T-1, T-2, \cdots, 1$$ $$i_{t}^{*}=\psi_{t+1}\left(i_{t+1}^{*}\right)$$求得最优路径$I^{*}=\left(i_{1}^{*}, i_{2}^{*}, \cdots, i_{T}^{*}\right)$}
		\end{algorithmic} 
	
	\end{algorithm}
	
	在实际应用中，往往不只是搜索一个最优状态序列.而是搜索$n$个最佳($n$-best)路径.因此,每个结点上常常需要记录$m$个最佳($m$-best，$m<n$)状态。
	
	
	
	\newpage
%	
%	
%	\subsection{最大熵模型MEM}
%	
%	\newpage
	

	\subsection{条件随机场CRF}
	
	\citep{lafferty2001conditional}最先将条件随机场用于命名识别。
	
	CRF的学习需要先了解马尔可夫网络及其因子分解\citep{李航2012统计学习方法},\citep{noauthor__nodate}。
	
	设$P(Y|X)$为线性条件随机场，则在随机变量$X$取值为$x$的条件下，随机变量$Y$取值为$y$的条件概率具有如下形式：\begin{equation}
P(y | x)=\frac{1}{Z(x)} \exp \left(\sum_{i, k} \lambda_{k} t_{k}\left(y_{i-1},y_{i}, x, i\right)+\sum_{i, l} \mu_{l} s_{l}\left(y_{i}, x, i\right)\right)
\label{eq:crf-1}
	\end{equation}
	其中，\begin{equation}
Z(x)=\sum_{y} \exp \left(\sum_{i, k} \lambda_{k} t_{k}\left(y_{i-1}, y_{i}, x, i\right)+\sum_{i, l} \mu_{i} s_{l}\left(y_{i}, x, i\right)\right)
\label{eq:crf-2}
	\end{equation}
	式中，$t_{k}$和$s_{l}$是特征函数，$\lambda_{k}$和$\mu_{l}$是对应的权值。$Z(x)$是规范化因子，求和是在所有可能的输出序列上进行的。
	
	式\eqref{eq:crf-1}和式\eqref{eq:crf-2}是线性链条件随机场模型的基本形式，表示给定输入
序列$x$，对输出序列$y$预测的条件概率。式\eqref{eq:crf-1}和式\eqref{eq:crf-2}中$t_{k}$是定义在边上的特征函数，称为转移特征，依赖于当前和前一个位置，$s_{l}$是定义在结点上的
特征函数，称为状态特征，依赖于当前位置。$t_{k}$和$s_{l}$都依赖于位置，是局部特征
函数。通常，特征函数$t_{k}$和$s_{l}$取值为1或0；当满足特征条件时取值为1，否则
为0。条件随机场完全由特征函数$t_{k}$，$s_{l}$和对应的权值$\lambda_{k}$，$\mu_{l}$确定。

	CRF也需要解决三个问题：
	\begin{enumerate}
		\item 特征的选取
		\item 参数训练：可在训练集上基于对数似然函数的最大化进行
		\item 解码
	\end{enumerate}
	
	\newpage
	
	\section{Semi-supervised learning approaches}
	
	半监督学习算法的主要技术是“bootstrapping”，这种算法涉及到一小部分的监督（它需要一组种子（seed）来启动学习过程）。
	
	比如，有一个目标是识别疾病名称的系统可能要求用户提供少量的示例名称。然后系统会搜索出包含这些名称的句子并且试着从这些句子中找到和疾病名称相关的上下文线索（contextual clues）。然后系统找出在类似的上下文中出现的名称。一直重复这个过程，最后找到大量的疾病名称和大量的上下文。
	
	可参考\citep{nadeau2006unsupervised},Nadeau et al. [52] proposed an unsupervised system for
	gazetteer building and named entity ambiguity resolution.
	This system combines entity extraction and disambiguation
	based on simple yet highly effective heuristics.
	Nadeau 等人 [ 52 ] 提出了一种无监督的地名录建筑系统和命名实体模糊解决方案。 该系统结合了基于简单而高效的启发式的实体提取和消除歧义。
	
	
	
	\section{Unsupervised learning approaches}
	
	一个典型的无监督学习的方法是聚类。主要是利用词汇资源（如WordNet）等进行上下文聚类。这些技术的关键思想是利用词汇资源、词汇模式和在大规模语料库上统计的信息来推断实体的指称项。
	
	Collins等人[51]指出，使用未标记的数据减少了对仅7个简单“种子”规则的监督要求。然后提出了两种命名实体分类的无监督算法。同样，Knowitall[7]系统利用一组谓词名称作为输入，并从小型通用提取模式中引导其识别过程。 
	
	 Zhang and Elhadad [41] proposed an unsupervised
	approach to extracting named entities from biomedical text.
	Instead of supervision, their model resorts to terminolo-
	gies, corpus statistics (e.g., inverse document frequency
	and context vectors) and shallow syntactic knowledge (e.g.,
	noun phrase chunking). Experiments on two mainstream
	biomedical datasets demonstrate the effectiveness and gen-
	eralizability of their unsupervised approach.此外，Zhang和Elhadad[41]提出了一种从生物医学文本中提取命名实体的无监督方法。它们的模型采用术语、语料库统计(如逆文档频率和上下文向量)和浅层句法知识(如名词短语分块)来代替监督。在两个主流生物医学数据集上的实验证明了其无监督方法的有效性和通用性。 
	
	
	
	
	
	
	
	
	\section{Deep-learning based approaches}
	
%	\subsection{BiLSTM-CRF\cite{bilstm-crf}}
	\subsection{BiLSTM-CRF}
%	\subsection{BiLSTM-CNNs\cite{bilstm-CNNs-ACL2016}}
	\subsection{BiLSTM-CNNs}

	% 我要试一下footcite\footnote{Zhou:2002:NER:1073083.1073163}
    \bibliography{mybib}
\end{document}